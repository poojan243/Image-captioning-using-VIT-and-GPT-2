# Image-captioning-using-VIT-and-GPT-2

This research project led to the development and publication of a significant paper, introducing a novel system designed to aid the visually impaired through multilingual, color-focused image captions. The study harnessed the advanced capabilities of the ViT and GPT-2 models, applying them to an extensive dataset of over 330,000 images from MS COCO and Flickr30k. This pioneering work bridged a crucial perceptual gap for visually impaired individuals and set a new benchmark in the integration of computer vision and natural language processing for improved image accessibility. The published paper has gained recognition in academic and professional communities for its innovative approach and its contribution to creating more inclusive technology through AI.
